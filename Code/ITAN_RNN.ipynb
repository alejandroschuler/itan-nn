{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo List\n",
    "1. V Implement params struct, save on train \n",
    "2. V Implement dynamic batch loading\n",
    "3. Normalize/document the dataset format\n",
    "4. Implement training with warm-start and progressive saving\n",
    "5. V Implement evaluation on trained models\n",
    "6. V Separate train/val split on skorch dataset (index list)\n",
    "7. Include time since admission as feature to predict LoS, else predict time until discharge on hourly (==LoS on admit)\n",
    "8. V Include sequence lengths, padding, and masking for LSTM model\n",
    "9. V Handle NaNs\n",
    "1. V Data flow diagram:  sources, expansions/extractions, dimensions\n",
    "2. V Time series prediction model:  training process and evaluation\n",
    "3. V Basic RNN trained\n",
    "4. V Preliminary results on extant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.0.1.post2\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Basic utilities\n",
    "import importlib\n",
    "import datetime\n",
    "import json\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import logging\n",
    "import argparse\n",
    "import os\n",
    "from pprint import pprint, pformat\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Basis Expansion (external package)\n",
    "from basis_expansions import (Binner, Polynomial, \n",
    "                              LinearSpline, CubicSpline,\n",
    "                              NaturalCubicSpline)\n",
    "from dftransformers import ColumnSelector, FeatureUnion, Intercept, MapFeature\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "print(\"PyTorch Version:  {}\".format(torch.__version__))\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import skorch\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.callbacks import Callback\n",
    "\n",
    "# Plotting and visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, Label, Layout\n",
    "from IPython.display import display, clear_output\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "# Local files\n",
    "import utils \n",
    "importlib.reload(utils)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "{'association_history': 0,\n",
      " 'batch_size': 1,\n",
      " 'cohort_features': ['ADM_LAPS2', 'ADM_COPS2', 'SEX', 'AGE'],\n",
      " 'cohort_file': 'itan_cohort_v.h5',\n",
      " 'cross_validation_ratios': {'test': 0.1, 'train': 0.8, 'val': 0.1},\n",
      " 'data_dir': 'Data',\n",
      " 'dropout_prob': 0.0,\n",
      " 'hourly_embedding_fill_value': 0,\n",
      " 'hourly_features': ['LAPS2'],\n",
      " 'label': 'LOS',\n",
      " 'learning_rate': 0.01,\n",
      " 'model_name': 'hierarchical_net',\n",
      " 'normalize_cohort': True,\n",
      " 'num_epochs': 5,\n",
      " 'num_hourly_output_features': 8,\n",
      " 'num_layers': 1,\n",
      " 'output_type': 'regression',\n",
      " 'pad_length': 12,\n",
      " 'pad_value': 0,\n",
      " 'patient_hourly_hidden_size': 8,\n",
      " 'predict_early': 0,\n",
      " 'project_dir': '..',\n",
      " 'random_seed': 7532941,\n",
      " 'sample_dir': 'Data/itan_hourly_encounter_splits',\n",
      " 'split_file': 'set_splits_all_FAC1A.json',\n",
      " 'subset': 0,\n",
      " 'subset_facility': 'FAC_1_A',\n",
      " 'unit_hourly_hidden_size1': 8,\n",
      " 'unit_hourly_hidden_size2': 8}\n",
      "Parameters set and saved to file\n"
     ]
    }
   ],
   "source": [
    "params = utils.Params(\n",
    "    {\n",
    "        \"output_type\": \"regression\",  # Classification vs Regression\n",
    "        \"project_dir\": \"..\",  # Top-level path for this project\n",
    "        \"data_dir\": \"Data\",  # Relative data path\n",
    "        \"cohort_file\": \"itan_cohort_v.h5\",  # File path to the cohort dataset (hdf5)\n",
    "        \"sample_dir\": \"Data/itan_hourly_encounter_splits\",  # Directory to the split patient hourly data\n",
    "        \"label\": \"LOS\",  # Column name of target/label in cohort dataframe\n",
    "        \"cohort_features\": [\"ADM_LAPS2\", \"ADM_COPS2\", \"SEX\", \"AGE\"],  # Which cohort-level features to use\n",
    "        \"hourly_features\": [\"LAPS2\"],#, \"IMAR_IM_GROUP\"],  # Which patient hourly features to use\n",
    "        \"pad_length\": 12,  # maximum number of hours of each time-series to include\n",
    "        \"pad_value\": 0,  # Value to pad variable-length input sequences with (unused), and to fill NaNs\n",
    "        \"hourly_embedding_fill_value\": 0,  # Value to pad variable-length input sequences with (unused), and to fill NaNs\n",
    "        \"predict_early\": 0,  # Number of hours to index LSTM output.  0 to use all available (max=sequence length)\n",
    "        \"association_history\": 0,  # How many hours to look back in unit occupancy to define patient associations\n",
    "        \"patient_hourly_hidden_size\": 8,  # hidden size of LSTM output\n",
    "        \"unit_hourly_hidden_size1\": 8,  # hidden size of LSTM output\n",
    "        \"unit_hourly_hidden_size2\": 8,  # hidden size of LSTM output\n",
    "        \"num_hourly_output_features\": 8,  # hidden size of LSTM output\n",
    "        \"num_layers\": 1,  # number of stacked LSTM layers (each with same hidden_size)\n",
    "        \"model_name\": \"hierarchical_net\",  # which model to train ('linear', 'lstm')\n",
    "        \"num_epochs\": 5,  # number of training epochs.  Note:  can rerun the training cell if warm_start=True\n",
    "        \"batch_size\": 1,  # number of samples per batch.  Usually limited by GPU memory.\n",
    "        \"learning_rate\": 0.01,  # optimizer learning rate\n",
    "        \"dropout_prob\": 0.0,  # dropout regularization probability (0 for not implemented)\n",
    "        \"cross_validation_ratios\": {\"train\": 0.8, \"val\": 0.1, \"test\": 0.1},  # Fraction of data in each CV split\n",
    "        \"split_file\": \"set_splits_all_FAC1A.json\",  # Where to save the CV splits\n",
    "        \"subset\": 0,  # train/evaluate on a smaller subset of the data for testing purposes, reduced runtime\n",
    "        \"subset_facility\": \"FAC_1_A\", # train/evaluate on a subset from a single facility, default None\n",
    "        \"random_seed\": 7532941,  # RNG seed for PyTorch (weight initialization and dataset shuffling per-epoch)\n",
    "        \"normalize_cohort\": True,  # Whether to normalize the cohort features (mean 0, std 1)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Where to save trained weights, log files, parameter file, results, etc.\n",
    "model_dir = os.path.join(params.project_dir, 'Code/experiments/', params.model_name)\n",
    "\n",
    "# Set the logger\n",
    "utils.set_logger(os.path.join(model_dir, 'ITAN_RNN.log'), logging.INFO) #  DEBUG  INFO\n",
    "\n",
    "# Set the PyTorch seed for shuffline and weight initialization reproducibility\n",
    "torch.manual_seed(params.random_seed)\n",
    "\n",
    "# Save params to file\n",
    "params.save(os.path.join(model_dir, 'params.json'))\n",
    "logging.info(\"Parameters:\\n{}\".format(pformat(params.__dict__)))\n",
    "logging.info(\"Parameters set and saved to file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Cross-Validation within each Facility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3da7fdef2ea45aa91e736450e1f7423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Facilities', max=10, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subsetted to 47384 samples in facility FAC_1_A only\n",
      "Building train split sample IDs\n",
      "Sorted samples:\n",
      "                            ADM_DATE  ENCOUNTER_ID\n",
      "31399   Fri Apr 01 00:00:00 GMT 2016  330000225236\n",
      "58460   Fri Apr 01 00:00:00 GMT 2016  330000774040\n",
      "41173   Fri Apr 01 00:00:00 GMT 2016  330000811143\n",
      "87780   Fri Apr 01 00:00:00 GMT 2016  330000103855\n",
      "197102  Fri Apr 01 00:00:00 GMT 2016  330000253566\n",
      "162096  Fri Apr 01 00:00:00 GMT 2016  330000016784\n",
      "201044  Fri Apr 01 00:00:00 GMT 2016  330000522750\n",
      "86013   Fri Apr 01 00:00:00 GMT 2016  330000986695\n",
      "85769   Fri Apr 01 00:00:00 GMT 2016  330000777166\n",
      "257755  Fri Apr 01 00:00:00 GMT 2016  330000678481\n",
      "train split:  31009 samples\n",
      "Building val split sample IDs\n",
      "Sorted samples:\n",
      "                            ADM_DATE  ENCOUNTER_ID\n",
      "234716  Tue May 24 00:00:00 GMT 2016  330000812759\n",
      "47862   Tue May 24 00:00:00 GMT 2016  330000639774\n",
      "47532   Tue May 24 00:00:00 GMT 2016  330000773481\n",
      "259183  Tue May 24 00:00:00 GMT 2016  330000200713\n",
      "94882   Tue May 24 00:00:00 GMT 2016  330000498326\n",
      "98394   Tue May 24 00:00:00 GMT 2016  330000462446\n",
      "249179  Tue May 24 00:00:00 GMT 2016  330000684203\n",
      "42761   Tue May 24 00:00:00 GMT 2016  330000608609\n",
      "115951  Tue May 24 00:00:00 GMT 2016  330000535480\n",
      "117994  Tue May 24 00:00:00 GMT 2016  330000670281\n",
      "val split:  4081 samples\n",
      "Building test split sample IDs\n",
      "Sorted samples:\n",
      "                            ADM_DATE  ENCOUNTER_ID\n",
      "96754   Wed Jan 10 00:00:00 GMT 2018  330000595742\n",
      "2463    Wed Jan 10 00:00:00 GMT 2018  330000792800\n",
      "65053   Wed Jan 10 00:00:00 GMT 2018  330000348041\n",
      "222359  Wed Jan 10 00:00:00 GMT 2018  330000655679\n",
      "38755   Wed Jan 10 00:00:00 GMT 2018  330000431119\n",
      "199269  Wed Jan 10 00:00:00 GMT 2018  330000213682\n",
      "253167  Wed Jan 10 00:00:00 GMT 2018  330000450495\n",
      "91158   Wed Jan 10 00:00:00 GMT 2018  330000730842\n",
      "91149   Wed Jan 10 00:00:00 GMT 2018  330000270937\n",
      "39805   Wed Jan 10 00:00:00 GMT 2018  330000814758\n",
      "test split:  4756 samples\n",
      "Saving set splits to ../Code/experiments/hierarchical_net/set_splits_all_FAC1A.json\n"
     ]
    }
   ],
   "source": [
    "# Load sample IDs and set up dataset\n",
    "cohort_df = pd.read_hdf(os.path.join(params.project_dir, params.data_dir, params.cohort_file),\n",
    "                                      start=0, \n",
    "                                      stop=-1)\n",
    "cohort_df.columns = [c.split(':')[1] for c in cohort_df.columns] # clean sim/src headers for fuzzed dataset\n",
    "\n",
    "# Check CV Parameters\n",
    "assert (sum([params.cross_validation_ratios[split] for split in params.cross_validation_ratios.keys()]) == 1), \"Cross validation splits must sum to 1:\\n{}\".format(params.cross_validation_ratios)\n",
    "\n",
    "# Split separately for each facility\n",
    "all_facilities = list(cohort_df[\"FAC_ID\"].unique())\n",
    "all_facilities.sort()\n",
    "cohort_df[\"CV_FAC_SPLIT\"] = 'none'\n",
    "with tqdm(total=len(all_facilities), desc=\"Facilities\") as t:\n",
    "    for facility in all_facilities:\n",
    "        logging.debug(\"Splitting patients for facility {}\".format(facility))\n",
    "        facility_df = cohort_df.loc[cohort_df[\"FAC_ID\"] == facility, :]\n",
    "\n",
    "        # Split indices for appropriate number/ratio of patients (not days) per split\n",
    "        N_patients = len(facility_df)\n",
    "        train_split_index = math.floor(N_patients * params.cross_validation_ratios[\"train\"])\n",
    "        val_split_index = train_split_index + math.floor(N_patients * params.cross_validation_ratios[\"val\"])\n",
    "\n",
    "        # Sort by admit date and find boundaries\n",
    "        facility_df = facility_df.sort_values([\"ADM_DATE\"]).reset_index(col_fill='cohort_index')\n",
    "        start_date = facility_df.loc[0, \"ADM_DATE\"]\n",
    "        end_date = facility_df.loc[N_patients-1, \"ADM_DATE\"]\n",
    "        train_boundary_date = facility_df.loc[train_split_index, \"ADM_DATE\"]\n",
    "        val_boundary_date = facility_df.loc[val_split_index, \"ADM_DATE\"]\n",
    "\n",
    "\n",
    "        # Assign to cross-validation splits\n",
    "        cohort_df.loc[((cohort_df[\"FAC_ID\"] == facility) &\n",
    "                       (cohort_df[\"DISCH_DATE\"] < train_boundary_date)), \"CV_FAC_SPLIT\"] = 'train'\n",
    "        cohort_df.loc[((cohort_df[\"FAC_ID\"] == facility) &\n",
    "                       (cohort_df[\"ADM_DATE\"] >= train_boundary_date) &\n",
    "                       (cohort_df[\"DISCH_DATE\"] < val_boundary_date)), \"CV_FAC_SPLIT\"] = 'val'\n",
    "        cohort_df.loc[((cohort_df[\"FAC_ID\"] == facility) &\n",
    "                       (cohort_df[\"ADM_DATE\"] >= val_boundary_date)), \"CV_FAC_SPLIT\"] = 'test'\n",
    "        t.update()\n",
    "\n",
    "logging.debug(\"\\nCross-validation split counts:\\n{}\\n\\nCross-validation split ratios:\\n{}\\n\".format(\n",
    "    cohort_df[\"CV_FAC_SPLIT\"].value_counts(),\n",
    "    cohort_df[\"CV_FAC_SPLIT\"].value_counts() / len(cohort_df)))\n",
    "\n",
    "\n",
    "# Restrict to samples with hourly data available\n",
    "hourly_sample_ids = [np.int64(f.split('.')[0].split('_')[-1])\n",
    "                     for f in os.listdir(os.path.join(params.project_dir, params.sample_dir))\n",
    "                     if f.endswith('.h5')]\n",
    "logging.debug(\"Loaded {} hourly sample IDs:\\n{}\".format(len(hourly_sample_ids), hourly_sample_ids[:3]))\n",
    "cohort_df = cohort_df.loc[cohort_df[\"ENCOUNTER_ID\"].astype(np.int64).isin(hourly_sample_ids), :]\n",
    "logging.debug(\"{} samples remaining after dropping samples which don't have hourly data\\n\".format(len(cohort_df)))\n",
    "\n",
    "# Subset to a single facility\n",
    "if params.subset_facility is not None:\n",
    "    cohort_df = cohort_df.loc[(cohort_df[\"FAC_ID\"] == params.subset_facility), :]\n",
    "    logging.info(\"Subsetted to {} samples in facility {} only\".format(len(cohort_df), params.subset_facility))\n",
    "\n",
    "# Split to lists of integers for constructing Skorch datasets\n",
    "cv_split_samples = {}\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    logging.info(\"Building {} split sample IDs\".format(split))\n",
    "    \n",
    "    # Select the matching cohort encount IDs\n",
    "    samples_df = cohort_df.loc[(cohort_df[\"CV_FAC_SPLIT\"] == split), [\"ADM_DATE\", \"ENCOUNTER_ID\"]]\n",
    "    \n",
    "    # Sort chronologically to maximize concurrency on subset\n",
    "    samples_df.sort_values(by=\"ADM_DATE\", ascending=True, inplace=True)\n",
    "    logging.info(\"Sorted samples:\\n{}\".format(samples_df[:10]))\n",
    "    \n",
    "    # Format as integers for size/index speed\n",
    "    cv_split_samples[split] = list(samples_df[\"ENCOUNTER_ID\"].astype(np.int64).reset_index(drop=True))\n",
    "    \n",
    "    # Select a smaller subset of each split for quick code testing\n",
    "    if params.subset > 0:\n",
    "        cv_split_samples[split] = cv_split_samples[split][:params.subset]\n",
    "    logging.info(\"{} split:  {} samples\".format(split, len(cv_split_samples[split])))\n",
    "\n",
    "\n",
    "# Save CV splits to file (for easier reload if using subsets, alternate datasets on different machines, etc.)\n",
    "split_filepath = os.path.join(params.project_dir, \"Code/\", \"experiments/\", params.model_name, params.split_file)\n",
    "logging.info(\"Saving set splits to {}\".format(split_filepath))\n",
    "with open(split_filepath, 'w') as f:\n",
    "    json.dump(cv_split_samples, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.debug(\"Defining dataset class\")\n",
    "class ITANDataset(skorch.dataset.Dataset):\n",
    "    def __init__(self, sample_ids, params):\n",
    "        \"\"\" Construct ITAN Dataset for dynamic data loading with Skorch\n",
    "        Args:\n",
    "            sample_ids: (pandas.Series or list)  Sample (encounter) ids for indexing cohort data and hourly files\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        self.sample_ids = sample_ids\n",
    "        logging.debug(\"Sample IDs:  {}\\n{}\".format(self.sample_ids.shape, self.sample_ids[:5]))\n",
    "        \n",
    "        # Keep all cohort data in memory\n",
    "        if params.cohort_features:\n",
    "            self.cohort_df = self.load_cohort_features(params)\n",
    "            self.cohort_features = [c for c in self.cohort_df.columns if c != params.label]\n",
    "            \n",
    "            # Drop samples that don't have cohort data\n",
    "            self.sample_ids = self.cohort_df.index\n",
    "            \n",
    "        # No hourly feature expansion implemented\n",
    "        self.hourly_features = params.hourly_features\n",
    "            \n",
    "        # Split features and label/target\n",
    "        self.Y = self.cohort_df.loc[:, [params.label]].values.astype(np.float32)\n",
    "        self.cohort_df.drop([params.label], axis=1, inplace=True)\n",
    "        \n",
    "        if params.normalize_cohort:\n",
    "            self.cohort_df = self.normalize(self.cohort_df)\n",
    "        \n",
    "        logging.debug(\"Final labels:  {}\\n{}\".format(self.Y.shape, self.Y[:3]))\n",
    "        logging.debug(\"Final cohort features:  {}\\n{}\".format(self.cohort_df.shape, self.cohort_df[:3]))\n",
    "\n",
    "    def normalize(self, df):\n",
    "        \"\"\" Normalize all columns in a dataframe (cohort_df) to mean 0 and std 1\n",
    "        \"\"\"\n",
    "        logging.debug(\"Normalizing features\")\n",
    "        scaler = StandardScaler()\n",
    "        x = df.values #returns a numpy array\n",
    "        x_scaled = scaler.fit_transform(x)\n",
    "        df[:] = x_scaled\n",
    "        return df\n",
    "            \n",
    "    def load_cohort_features(self, params):\n",
    "        \"\"\" Load all cohort-level features from file and preprocess\n",
    "        Args:\n",
    "            params:  dictionary of parameters\n",
    "        Return:\n",
    "            cohort_df:  (pandas.DataFrame)  All cohort-level data indexed by encounter ID\n",
    "        \"\"\"\n",
    "        logging.debug(\"Loading cohort features\")\n",
    "        cohort_df = pd.read_hdf(os.path.join(params.project_dir, params.data_dir, params.cohort_file),\n",
    "                                start=0, \n",
    "                                stop=-1) # note: loading all features and dropping after cleaning sim/src\n",
    "        cohort_df.columns = [c.split(':')[1] for c in cohort_df.columns] # clean sim/src headers for fuzzed dataset\n",
    "        cohort_df.set_index(\"ENCOUNTER_ID\", inplace=True)\n",
    "        cohort_df = cohort_df.loc[:, [params.label]+params.cohort_features]\n",
    "        logging.debug(\"Loaded {} samples from cohort-level data\".format(cohort_df.shape))\n",
    "        \n",
    "        # Drop rows with duplicate Encounter IDs\n",
    "        cohort_df = cohort_df[~cohort_df.index.duplicated(keep='first')]\n",
    "        logging.debug(\"{} samples after dropping duplicate encounter IDs\".format(cohort_df.shape))\n",
    "        \n",
    "        # Select out the rows for encounter IDs in the current CV split\n",
    "        cohort_df = cohort_df.loc[self.sample_ids, :]\n",
    "        logging.debug(\"Loaded cohort data:  {}\\n{}\".format(cohort_df.shape, cohort_df[:5]))\n",
    "        logging.debug(\"Cohort datatypes:\\n{}\".format(cohort_df.dtypes))\n",
    "        \n",
    "        # Drop rows with NaN\n",
    "        cohort_df.dropna(axis='index', how='any', inplace=True)\n",
    "        logging.debug(\"{} samples after dropping rows with missing values\".format(len(cohort_df)))\n",
    "        \n",
    "        # Expand categorical features with dummy indicators\n",
    "        categorical_features = [c for ci, c in enumerate(cohort_df.columns) if cohort_df.dtypes[ci]==object]\n",
    "        if categorical_features:\n",
    "            logging.debug(\"Expanding categorical variables to dummy indicators:  {}\".format(categorical_features))\n",
    "            cohort_df = pd.get_dummies(cohort_df, columns=categorical_features).astype(np.float32)\n",
    "            logging.debug(\"Expanded/binarized data:\\n{}\".format(cohort_df[:5]))\n",
    "        else:\n",
    "            logging.debug(\"No categorical variables to expand\")\n",
    "        \n",
    "        # Return the expanded cohort data, with labels (targets, Y) included\n",
    "        return cohort_df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sample_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "#         logging.debug(\"getitem index: {}\".format(index))\n",
    "        if self.hourly_features:\n",
    "            filepath = os.path.join(self.params.project_dir,\n",
    "                                    self.params.sample_dir,\n",
    "                                    \"itan_hourly_enc_{}.h5\".format(self.sample_ids[index]))\n",
    "\n",
    "    #         logging.debug(\"Sample cohort data:  {},   {}\".format(sample_cohort_df.shape, sample_cohort_df))\n",
    "\n",
    "            # Dynamically load patient hourly data\n",
    "            sample_hourly_df = pd.read_hdf(filepath, key=\"hourly\").reset_index()\n",
    "            sample_hourly_df.drop('index', axis=1, inplace=True)\n",
    "            sample_hourly_df.columns = [c.split(':')[1] for c in sample_hourly_df.columns] # clean sim/src headers for fuzzed dataset\n",
    "#         logging.debug(\"Sample hourly data:  {},   {}\".format(sample_hourly_df.shape, sample_hourly_df.columns))\n",
    "            sample_pad = min(params.pad_length, len(sample_hourly_df))\n",
    "        else:\n",
    "            # No hourly features, don't tile cohort features\n",
    "            sample_pad = 1\n",
    "\n",
    "        # Construct feature array:  sklearn/skorch LSTM format is batch-first:  (batch, sequence, feature)\n",
    "        # Pad all sequences to fixed length\n",
    "        Xi = np.full(shape=(params.pad_length, len(self.hourly_features)+len(self.cohort_features)),\n",
    "                     fill_value=params.pad_value,\n",
    "                     dtype=np.float32)\n",
    "#         logging.debug(\"Xi init: {}\".format(Xi.shape))\n",
    "        \n",
    "        # Fill with hourly data\n",
    "        if self.hourly_features:\n",
    "            sample_hourly_df.fillna(params.pad_value, inplace=True)\n",
    "            Xi[:(sample_pad), :len(self.hourly_features)] = sample_hourly_df.loc[:(sample_pad-1), self.hourly_features].values\n",
    "        \n",
    "        # Fill with cohort data tiled on sequency (temporal) dimension\n",
    "        if self.cohort_features:\n",
    "            sample_cohort_df = self.cohort_df.loc[[self.sample_ids[index]], :].iloc[[0], :]\n",
    "#             logging.debug(\"sample cohort df: {}\\n{}\".format(sample_cohort_df.shape, sample_cohort_df))\n",
    "#             logging.debug(\"tiled cohort df:  {}\".format(np.tile(sample_cohort_df.values, (sample_pad, 1)).shape))\n",
    "            Xi[:(sample_pad), len(self.hourly_features):] = np.tile(sample_cohort_df.values, (sample_pad, 1))\n",
    "        \n",
    "        # Optionally reset the sequence length to early-output\n",
    "        # Most likely used in evaluation after training on full length\n",
    "        # Leave at 0 to make prediction at end of sequence\n",
    "        if self.params.predict_early > 0:\n",
    "            output_index = min(self.params.predict_early, sample_pad)\n",
    "        else:\n",
    "            output_index = sample_pad\n",
    "            \n",
    "        yi = self.Y[index]\n",
    "        return (Xi, output_index), yi\n",
    "\n",
    "    \n",
    "# Construct Skorch datasets for training\n",
    "logging.info(\"Constructing datasets for each cross-validation split.\")\n",
    "datasets = {}\n",
    "for split in [\"train\", \"val\"]:\n",
    "    datasets[split] = ITANDataset(cv_split_samples[split], params)\n",
    "    logging.info(\"{} split:  {} samples:\\n{}\\nX: {}\\nY: {}\".format(split, len(cv_split_samples[split]), cv_split_samples[split][:3], None, datasets[split].Y.shape))\n",
    "    \n",
    "    logging.info(\"y values:  {}:  mean {:.2f} +/- {:.2f}\".format(\n",
    "        datasets[split].Y.shape,\n",
    "        np.mean(datasets[split].Y),\n",
    "        np.std(datasets[split].Y)))  \n",
    "    \n",
    "    # Test iteration through dataset\n",
    "    if False:\n",
    "        with tqdm(total=len(datasets[split]), desc=\"Testing dataset iterator\") as st:\n",
    "            for sample in datasets[split]:\n",
    "                st.update()\n",
    "logging.info(\"\\nDatasets defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model defined!\n"
     ]
    }
   ],
   "source": [
    "# Neural Network definition\n",
    "class LinearRegressorModule(nn.Module):\n",
    "    def __init__(self, num_units=10, nonlin=F.relu):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define network layers\n",
    "        self.denseX = nn.Linear(len(params.hourly_features)+len(params.cohort_features)+1, 1) #*params.pad_length\n",
    "        \n",
    "        # Manual initialization\n",
    "        self.denseX.weight.data.fill_(0.0)\n",
    "        self.denseX.bias.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        shape_debug = False\n",
    "        if shape_debug: logging.info(\"input: {}\\n{}\".format(X.size(), X.dtype))\n",
    "        X = torch.mean(X, dim=1, keepdim=False)\n",
    "        if shape_debug: logging.info(\"reshape: {}\".format(X.size()))\n",
    "        X = self.denseX(X)\n",
    "        if shape_debug: logging.info(\"output: {}\".format(X.size()))\n",
    "        return X\n",
    "    \n",
    "\n",
    "# Recurrent Neural Network definition\n",
    "class LSTMRegressorModule(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        input_size = len(params.hourly_features)+len(params.cohort_features)+1\n",
    "        hidden_size = params.hidden_size\n",
    "        \n",
    "        # Define network layers\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.output_regression = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "#         self.bn1 = \n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        shape_debug = False\n",
    "        sequences, lengths = inputs\n",
    "        if shape_debug: logging.info(\"input:  {}\".format(sequences.size()))\n",
    "        if shape_debug: logging.info(\"lengths:  {}\".format(lengths.size()))\n",
    "        \n",
    "        output, (h_n, c_n) = self.lstm1(sequences)\n",
    "        X = output\n",
    "        if shape_debug: logging.info(\"LSTM 1: {}\".format(X.size()))\n",
    "            \n",
    "        length_indices = lengths.view(-1, 1, 1).expand(X.size(0), 1, X.size(2)).long() - 1\n",
    "        if shape_debug: logging.info(\"Slice Indices:  {}\".format(length_indices.size()))\n",
    "#         if shape_debug: logging.info(\"Slice Indices:  {}\\n{}\".format(length_indices.size(), length_indices))\n",
    "        \n",
    "#         X = torch.index_select(output, dim=1, index=lengths)\n",
    "        X = X.gather(1, length_indices)\n",
    "        if shape_debug: logging.info(\"Length-indexed outputs:  {}\".format(X.size()))\n",
    "            \n",
    "        X = X.squeeze(1)\n",
    "        if shape_debug: logging.info(\"Last Output in Sequence:  {}\\n{}\".format(X.size(), X.dtype))\n",
    "        \n",
    "        X = self.output_regression(X)\n",
    "        if shape_debug: logging.info(\"Output: {}\".format(X.size()))\n",
    "        return X\n",
    "    \n",
    "    \n",
    "class ITANStrainNetworkModule(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        \"\"\" Load parameters and initialize NN model\n",
    "        \n",
    "        Params:\n",
    "            output_type:  (string) target type, determines output layer\n",
    "            include_strain_variables:  (bool) whether to use the unit and network-level strain connections\n",
    "            hourly_features:  (list) list of hourly feature column names\n",
    "            cohort_features:  (list) list of cohort-level (on admission) feature column names\n",
    "            patient_hourly_hidden_size:  (int) hidden size of LSTM for all batch-associated patient hourly features\n",
    "            unit_hourly_hidden_size:  (int) hidden size of set accumulator for embedding all patients simultaneously occupying a unit\n",
    "            num_hourly_output_features:  (int)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Compute and save parameters\n",
    "        self.params = params\n",
    "        self.input_size = len(params.hourly_features)+len(params.cohort_features)+1\n",
    "        self.num_output_features = params.num_hourly_output_features+len(params.cohort_features)\n",
    "        \n",
    "        # Define internal network layers and RNN cells\n",
    "        self.lstm_associated_patient = nn.LSTM(input_size=self.input_size,\n",
    "                                               hidden_size=self.params.patient_hourly_hidden_size,\n",
    "                                               num_layers=1,\n",
    "                                               batch_first=True,\n",
    "                                               bidirectional=False)\n",
    "        \n",
    "        self.patient_set_accumulate = nn.LSTM(input_size=self.params.patient_hourly_hidden_size,\n",
    "                                              hidden_size=self.params.unit_hourly_hidden_size1,\n",
    "                                              num_layers=1,\n",
    "                                              batch_first=True,\n",
    "                                              dropout=params.dropout_prob,\n",
    "                                              bidirectional=False)#True)\n",
    "        \n",
    "        self.lstm_unit = nn.LSTM(input_size=self.params.unit_hourly_hidden_size1,\n",
    "                                 hidden_size=self.params.unit_hourly_hidden_size2,\n",
    "                                 num_layers=1,\n",
    "                                 batch_first=True,\n",
    "                                 bidirectional=False)\n",
    "        \n",
    "        self.lstm_sample_patient = nn.LSTM(input_size=self.params.patient_hourly_hidden_size+len(self.params.cohort_features),\n",
    "                                           hidden_size=self.params.num_hourly_output_features,\n",
    "                                           num_layers=1,\n",
    "                                           batch_first=True,\n",
    "                                           bidirectional=False)\n",
    "        \n",
    "        # Define output layer\n",
    "        if params.output_type == \"classification\":\n",
    "            self.output_layer = nn.Softmax(self.num_output_features, 2)\n",
    "        elif params.output_type == \"regression\":\n",
    "            self.output_layer = nn.Linear(self.num_output_features, 1)\n",
    "        else:\n",
    "            logging.warning(\"Unknown output type:  {}\".format(self.output_layer))\n",
    "            \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Inputs:  tuple built by data_loader\n",
    "        \n",
    "        Def:  Batch patients/samples:  Exactly batch_size randomly selected set of patients to make an output prediction for the current batch.  \n",
    "        Def:  Patients associated with batch:  Unknown number of patients who were in any unit that one of the batch patients eventually occupied\n",
    "              This allows for building a unit history prior to the batch patients' occupancy\n",
    "              Always at least batch_size, additional patients may be clipped to a randomly selected subset for size restrictions and regularization\n",
    "        Def:  pad_length:  maximum number of hours of data for any patient in the batch\n",
    "        Def:  time_max:  Total number of hours from first sample of any batch-associated patient to last sample of any\n",
    "        \n",
    "        Args:\n",
    "            inputs:  (tuple)  all batch data generated per-sample by data loader and batched by Skorch\n",
    "            batch_samples:  (batch_patients)  boolean mask for which patients are being predicted out of all patients associated with the batch\n",
    "                            sums to batch_size\n",
    "            patient_inputs:  (tuple)  all data for patients associated with the batch\n",
    "            \n",
    "            patient_static_features:  (batch_patients x num_patient_static_features)  static features for all patients associated with the batch\n",
    "            Xpatient_locations:  (batch_patients x time_max x num_units)  location of each patient associated with the batch at each time (global time)\n",
    "            unit_hourly_occupants:  (batch_units x time_max x max_occupancy)  indices into batch patients for all occupants of each unit at each global time\n",
    "            patient_hourly_features:  (batch_patients x pad_length x num_patient_hourly_features)  full history of patient hourly features for each patient associated with batch\n",
    "            patient_hourly_lengths:  (batch_patients)  number of valid samples in hourly sequences (clipped to pad_length)\n",
    "            patient_hourly_timemask:  (batch_patients x time_max)  boolean mask to map personal time indices to global time index\n",
    "        \"\"\"\n",
    "        shape_debug = True\n",
    "        # Extract bundled batch data\n",
    "        patient_static_features, patient_hourly_locations, patient_hourly_features, patient_hourly_lengths = inputs\n",
    "        \n",
    "        if shape_debug: logging.info(\"Patient Static Features: {}\".format(patient_static_features.size()))\n",
    "        if shape_debug: logging.info(\"Patient Hourly Locations: {}\".format(patient_hourly_locations.size()))\n",
    "        if shape_debug: logging.info(\"Patient Hourly Features: {}\".format(patient_hourly_features.size()))\n",
    "        if shape_debug: logging.info(\"Patient Hourly Lengths: {}\".format(patient_hourly_lengths.size()))\n",
    "        \n",
    "        # Wrap up time-history for each patient associated with batch\n",
    "        # Keep full RNN history (1-directional)\n",
    "        patient_personal_hourly_embeddings, _ = self.lstm_associated_patient(patient_hourly_features)  # (batch_patients x pad_length x patient_hourly_hidden_size)\n",
    "        if shape_debug: logging.info(\"Patient Hourly Embeddings personal-time: {}\".format(patient_personal_hourly_embeddings.size()))\n",
    "\n",
    "        # Get batch-associated patient hourly embeddings on global time index, index out patients for each unit at each time\n",
    "        # The shape of mask must be broadcastable with the shape of the underlying tensor.\n",
    "        patient_global_hourly_embeddings = torch.new_full(size=(self.params.time_max, self.params.patient_hourly_hidden_size1), fill_value=self.params.hourly_embedding_fill_value, dtype=bool)\n",
    "        patient_global_hourly_embeddings.masked_scatter_(patient_hourly_timemask, patient_personal_hourly_embeddings)  # (batch_patients x time_max x patient_hourly_hidden_size)\n",
    "        if shape_debug: logging.info(\"Patient Hourly Embeddings global-time: {}\".format(patient_global_hourly_embeddings.size()))\n",
    "            \n",
    "        # Group batch-associated patient hourly embeddings by simultaneous unit occupancy and expand dimension appropriately\n",
    "        # gather:  index shape is same as output shape (but broadcast???)\n",
    "        # index provided by data loader\n",
    "        unit_patient_hourly_embeddings = torch.gather(patient_hourly_embeddings, dim=0)#, unit_hourly_occupants)  # (batch_units x time_max x max_occupancy x patient_hourly_hidden_size)\n",
    "        if shape_debug: logging.info(\"Unit-Patient Hourly Embeddings reshaping: {}\".format(unit_patient_hourly_embeddings.size()))\n",
    "    \n",
    "        # Merge hourly embeddings over all batch-associated patients in a unit simultaneously\n",
    "        # Can be done with reduce-max, RNN, shuffle/RNN, or search literature for unordered set combination methods\n",
    "        # note:  torch.matmul multiplies last 2 dimensions, all preceding are considered as batch\n",
    "        # note:  may need to re-.view() so batch_size and max_time are both in batch dimension, and reshape after\n",
    "        unit_patient_hourly_embeddings = unit_patient_hourly_embeddings.view(-1, max_occupancy, patient_hourly_hidden_size)  # (batch_units*time_max x max_occupancy x patient_hourly_hidden_size)\n",
    "        output, (h_n, c_n) = self.patient_set_accumulate()  \n",
    "        occupancy_indices = unit_hourly_occupants.view(-1, 1, 1).expand(output.size(0), 1, output.size(2)).long() - 1\n",
    "        unit_hourly_embeddings = output.gather(1, occupancy_indices) # (batch_units*time_max x unit_hourly_hidden_size)\n",
    "        unit_hourly_embeddings = unit_hourly_embeddings.view(batch_units, time_max, unit_hourly_hidden_size)  # (batch_units x time_max x unit_hourly_hidden_size)\n",
    "        if shape_debug: logging.info(\"Unit Hourly Embeddings patient-wrap: {}\".format(unit_hourly_embeddings.size()))\n",
    "            \n",
    "        # Wrap up time-history for each unit associated with batch\n",
    "        # Unit summaries change as patients move in and out\n",
    "        # Keep full history\n",
    "        unit_accumulated_embeddings, _ = self.lstm_unit(unit_hourly_embeddings)  # (batch_units, time_max, unit_hourly_hidden_size2)\n",
    "        if shape_debug: logging.info(\"Unit Hourly Embeddings time-wrap: {}\".format(unit_accumulated_embeddings.size()))\n",
    "        \n",
    "        # Select out the unit-level data for each batch patient at each global time\n",
    "        # Duplicate data as necessary\n",
    "        patient_global_hourly_unit_features = torch.gather(unit_accumulated_embeddings)#, dim=0, patient_locations)  # (batch_samples x pad_length x unit_hourly_hidden_size)\n",
    "        if shape_debug: logging.info(\"Patient Unit Hourly Embeddings reshaping/duplication in global-time: {}\".format(patient_global_hourly_unit_features.size()))\n",
    "        \n",
    "        # Re-index to personal (sample) time for each batch patient\n",
    "        # At each personal time step in patient_sequences, get patient location, index the accumulated unit data, and concat to row for patient/time\n",
    "        patient_personal_hourly_unit_features = torch.gather(patient_global_hourly_unit_features, dim=1)#, patient_hourly_timemask)\n",
    "        if shape_debug: logging.info(\"Patient Unit Hourly Embeddings in personal-time: {}\".format(patient_personal_hourly_unit_features.size()))\n",
    "            \n",
    "        # Merge patient hourly and the unit data for their current location\n",
    "        patient_hourly_features = concat(patient_sequences, patient_hourly_unit_features)  # (batch_samples x pad_length x unit_hourly_hidden_size+num_hourly_features)\n",
    "        if shape_debug: logging.info(\"Patient Hourly Features (unit & personal): {}\".format(patient_hourly_features.size()))\n",
    "        \n",
    "        # Wrap up time-history for each patient as they move through units\n",
    "        patient_accumulated_features = self.lstm_sample_patient(patient_data_inst, patient_lengths)  # (batch_samples x pad_length x patient_hourly_hidden_size2)\n",
    "        if shape_debug: logging.info(\"Patient Hourly Embeddings (wrap all features over personal time): {}\".format(patient_accumulated_features.size()))\n",
    "            \n",
    "        # Tile and append patient static features\n",
    "        patient_all_features = concat(patient_accumulated_features, torch.tile(patient_static_features))  # (batch_samples x pad_length x patient_hourly_hidden_size2+num_static_features)\n",
    "        if shape_debug: logging.info(\"All Patient Features (hourly embeddings and static features): {}\".format(patient_all_features.size()))\n",
    "            \n",
    "        # Make a prediction (regression/classification) for each patient in batch (at each time-elapsedLoS?)\n",
    "        patient_hourly_pred = self.output_layer(patient_data_accumulate)\n",
    "        if shape_debug: logging.info(\"Patient Predictions at each personal-timestep: {}\".format(patient_hourly_pred.size()))\n",
    "        return patient_hourly_pred\n",
    "    \n",
    "    \n",
    "# Select a model to train\n",
    "if params.model_name == \"linear\":\n",
    "    nn_module = LinearRegressorModule\n",
    "elif params.model_name == \"lstm\":\n",
    "    nn_module = LSTMRegressorModule\n",
    "elif params.model_name == \"hierarchical_net\":\n",
    "    nn_module = HierarchicalNetworkModule\n",
    "else:\n",
    "    print(\"Unknown model {}, cannot construct\".format(params.model))\n",
    "    raise NotImplementedError\n",
    "\n",
    "    \n",
    "# # Evaluation Metrics\n",
    "# class R_squared(Callback):\n",
    "#     def on_epoch_end(dataset_train, dataset_val):\n",
    "#         y_train_pred = self.predict(dataset_train)\n",
    "#         y_val_pred = self.predict(dataset_val)\n",
    "        \n",
    "#         r2_train = r2_score(dataset_train.Y, y_train_pred)\n",
    "#         r2_val = r2_score(dataset_val.Y, y_val_pred)\n",
    "\n",
    "# callbacks = [\n",
    "#     (\"R-squared\", R_squared())\n",
    "# ]\n",
    "\n",
    "callbacks = [\n",
    "    skorch.callbacks.ProgressBar()\n",
    "]\n",
    "\n",
    "\n",
    "# NN Regression setup.  Construct model with preset parameters\n",
    "net = NeuralNetRegressor(\n",
    "    nn_module(params),\n",
    "    max_epochs=params.num_epochs,\n",
    "    lr=params.learning_rate,\n",
    "    batch_size=params.batch_size,\n",
    "    callbacks=callbacks,\n",
    "    # Shuffle training data on each epoch\n",
    "    iterator_train__shuffle=True,\n",
    "    verbose=1,\n",
    "    warm_start=True,\n",
    ")\n",
    "\n",
    "# Use sklearn pipeline for preprocessing, cross-validation, etc.\n",
    "# Any preprocessing must be compatible with Skorch dataset and 3d input (batched multifeature timeseries)\n",
    "# pipe = Pipeline([\n",
    "#         ('scale', StandardScaler()),  # note: doesnt work with multidimensional input\n",
    "#         ('nnet', net)])\n",
    "# model = pipe\n",
    "model = net\n",
    "logging.info(\"Model defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "Note: can rerun this cell with warm-start to continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6843f7cdb55244c2b50712e305041307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "batch must contain tensors, numbers, dicts or lists; found object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-bbca5235c439>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fitting model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fit complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.5/site-packages/skorch/regressor.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# this is actually a pylint bug:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# https://github.com/PyCQA/pylint/issues/1085\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNeuralNetRegressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.5/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.5/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, classes, **fit_params)\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_train_begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.5/site-packages/skorch/net.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(self, X, y, epochs, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_epoch_begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mon_epoch_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mXi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m                 \u001b[0myi_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myi\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0my_train_is_ph\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_batch_begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myi_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/python3/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;31m# array of string classes and object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[SaUO]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: batch must contain tensors, numbers, dicts or lists; found object"
     ]
    }
   ],
   "source": [
    "# Load parameters from file\n",
    "# params.model_name = 'test'  # optionally override the model to train\n",
    "# params = utils.Params(os.path.join(params.project_dir, 'Code/experiments/', params.model_name, 'params.json'))\n",
    "params.update(os.path.join(params.project_dir, 'Code/experiments/', params.model_name, 'params.json'))\n",
    "\n",
    "\n",
    "# Reload weights if necessary\n",
    "# hot_start = False\n",
    "# if hot_start:\n",
    "#     logging.info(\"Hot-starting model with trained weights from {}\".format(model_dir))\n",
    "#     with open(os.path.join(model_dir, 'trained_weights.pkl'), 'rb') as f:\n",
    "#         model = pickle.load(f)\n",
    "# else:\n",
    "#     logging.info(\"Re-initializing model from scratch\")\n",
    "\n",
    "# Use CUDA if available\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "logging.info(\"Fitting model...\")\n",
    "model.fit(datasets[\"train\"], y=None)\n",
    "logging.info(\"Fit complete!\")\n",
    "\n",
    "# Save trained weights\n",
    "# with open(os.path.join(model_dir, 'trained_weights.pkl'), 'wb') as f:\n",
    "#     pickle.dump(model, f)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "# Log results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate fitted model on training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Evaluating fitted model on training and validation sets\")\n",
    "print(\"y_train:       {}:  mean {:.2f} +/- {:.2f}\".format(\n",
    "    datasets[\"train\"].Y.shape,\n",
    "    np.mean(datasets[\"train\"].Y),\n",
    "    np.std(datasets[\"train\"].Y)))    \n",
    "y_train_pred = model.predict(datasets[\"train\"])\n",
    "print(\"y_train_pred:  {}:  mean {:.2f} +/- {:.2f}\".format(\n",
    "    y_train_pred.shape,\n",
    "    np.mean(y_train_pred),\n",
    "    np.std(y_train_pred)))\n",
    "r2_train = r2_score(datasets[\"train\"].Y, y_train_pred)\n",
    "print(\"R-squared (train):  {:.4f}\".format(r2_train))\n",
    "\n",
    "y_val_pred = model.predict(datasets[\"val\"])\n",
    "r2_val = r2_score(datasets[\"val\"].Y, y_val_pred)\n",
    "print(\"R-squared (val):    {:.4f}\".format(r2_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Variable-Length Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize arrays for plotting\n",
    "hour_range = np.arange(1, params.pad_length+1)\n",
    "logging.info(\"Evaluating trained model with {} to {} hours of input data\".format(hour_range[0], hour_range[-1]))\n",
    "r2_train = np.zeros(params.pad_length, dtype=np.float32)\n",
    "r2_val = np.zeros(params.pad_length, dtype=np.float32)\n",
    "\n",
    "# Run inference on pretrained model with each early-predict time\n",
    "with tqdm(total=params.pad_length, desc=\"Early Predict Hours\") as lt:\n",
    "    for li, input_length in enumerate(hour_range):\n",
    "        # Set parameters for early evaluation\n",
    "        datasets[\"train\"].params.predict_early = input_length\n",
    "        datasets[\"val\"].params.predict_early = input_length\n",
    "        \n",
    "        # Run new inferences on rebuilt datasets using pretrained model\n",
    "        logging.debug(\"Running inferences with pretrained models\")\n",
    "        y_train_pred = model.predict(datasets[\"train\"])\n",
    "        r2_train[li] = r2_score(datasets[\"train\"].Y, y_train_pred)\n",
    "        y_val_pred = model.predict(datasets[\"val\"])\n",
    "        r2_val[li] = r2_score(datasets[\"val\"].Y, y_val_pred)\n",
    "        lt.update()\n",
    "        logging.info(\"R-squared at {:<3} hours:\\ttrain = {:.4f}, val = {:.4f}\".format(input_length, r2_train[li], r2_val[li]))\n",
    "\n",
    "# Cleanup parameters\n",
    "datasets[\"train\"].params.hourly_features = params.hourly_features\n",
    "datasets[\"train\"].params.predict_early = params.predict_early\n",
    "datasets[\"val\"].params.hourly_features = params.hourly_features\n",
    "datasets[\"val\"].params.predict_early = params.predict_early"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_stop = 6\n",
    "\n",
    "logging.info(\"R-squared at {:<3} hours:\\ttrain = {:.4f}, val = {:.4f}\".format(input_length, r2_train[li], r2_val[li]))\n",
    "# Plot results for evaluation with variable inputs\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 5), squeeze=False)\n",
    "axes[0, 0].plot(hour_range[:plot_stop], r2_train[:len(hour_range)][:plot_stop], color='red', label='train')\n",
    "axes[0, 0].plot(hour_range[:plot_stop], r2_val[:len(hour_range)][:plot_stop], color='blue', label='val')\n",
    "axes[0, 0].set_ylabel(\"R-squared\")\n",
    "axes[0, 0].set_title(\"Model Evaluation with Incoming Data\")\n",
    "axes[0, 0].set_xlabel(\"Number of Input Hours\")\n",
    "axes[0, 0].legend(loc='upper left')\n",
    "axes[0, 0].grid(True)\n",
    "fig.savefig(\"RNN_evaluation_over_time_alldata.png\", tight_layout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Batch Loader for Strain Network\n",
    "\n",
    "Notes:\n",
    "1.  Batches are always drawn from same facility\n",
    "2.  Try to draw batches in close temporal proximity\n",
    "3.  Perform mini-epochs for each facility?  Could have cyclical training issues, but much more efficient memory-wise.\n",
    "4.  Hold batch-associated info in memory?\n",
    "\n",
    "\n",
    "High-Memory Method:\n",
    "1.  1 facility at a time\n",
    "2.  Hold all data in CPU memory\n",
    "3.  Move all hourly data into sparse, 3d array on global timescale (num_patients x time_max x num_features)\n",
    "4.  Move only necessary hourly data into GPU memory on batching?  Need to control batching indices if so to get all batch-associated patients.\n",
    "\n",
    "Default solution:\n",
    "To override batching, need to control torch.utils.data.DataLoader() for train_iterator and valid_iterator in NN\n",
    "Pass collate_fn() to DataLoader():  merges a list of samples to form a mini-batch.\n",
    "\n",
    "Alternative solution:\n",
    "Keep everything in memory, don't have any strain vars in batch, just the sampling indices.  \n",
    "Then use the data on forward(), instead of __getitem__()\n",
    "so the data must be available to the network module init, rather than just the dataloader init,\n",
    "unless a handler/pointer is passed rather than the data\n",
    "\n",
    "Easiest Method:\n",
    "Duplicate all necessary strain data per-sample within batch, keep batch size 1 or small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a2c4ec6abe466d93d29fa2d048c059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Sample IDs', max=10, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c845f552464ea2b1a6b0c84f355edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Sample IDs', max=10, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'sample_hourly_locations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-2ae38202e1db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0mtest_sample_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m330000197395\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m330000372959\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m330000804075\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m330000699855\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m330000920644\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m330000322138\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m330000881688\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m330000563672\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m330000535940\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m330000503459\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mITANStrainDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sample_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m \u001b[0mtest_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset created!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-2ae38202e1db>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# Bundle all arrays/values for batching, and return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msample_static_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_hourly_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_hourly_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, sample_hourly_lengths)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_hourly_locations' is not defined"
     ]
    }
   ],
   "source": [
    "class ITANStrainDataset(skorch.dataset.Dataset):\n",
    "    def __init__(self, sample_ids, params):\n",
    "        \"\"\" Construct ITAN Dataset for dynamic data loading with Skorch\n",
    "        Args:\n",
    "            sample_ids: (pandas.Series or list)  Sample (encounter) ids for indexing cohort data and hourly files\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        self.sample_ids = sample_ids\n",
    "        logging.debug(\"Sample IDs:  {}\\n{}\".format(self.sample_ids.shape, self.sample_ids[:5]))\n",
    "        \n",
    "        # Keep all cohort data in memory\n",
    "        if params.cohort_features:\n",
    "            self.cohort_df = self.load_cohort_features()\n",
    "            self.cohort_features = [c for c in self.cohort_df.columns if c != params.label]\n",
    "            \n",
    "            # Drop samples that don't have cohort data\n",
    "            self.sample_ids = self.cohort_df.index\n",
    "            \n",
    "        # Split features and label/target\n",
    "        self.Y = self.cohort_df.loc[:, [params.label]].values.astype(np.float32)\n",
    "        self.cohort_df.drop([params.label], axis=1, inplace=True)\n",
    "        \n",
    "        if params.normalize_cohort:\n",
    "            self.cohort_df = self.normalize(self.cohort_df)\n",
    "            \n",
    "        # Keep all hourly data in memory\n",
    "        if params.hourly_features:\n",
    "            self.hourly_features = params.hourly_features\n",
    "            self.hourly_dataframes, self.global_time_indices = self.load_hourly_features(self.sample_ids)\n",
    "            \n",
    "        # Prebuild all patient locations\n",
    "        self.hourly_patient_locations = self.prebuild_patient_locations(self.sample_ids, self.hourly_dataframes, self.global_time_indices)\n",
    "        \n",
    "        # Prebuild all patient associations to index at batch-time\n",
    "        self.patient_associations = self.prebuild_patient_associations(self.sample_ids, self.hourly_patient_locations, history_hours=params.association_history)\n",
    "\n",
    "        \n",
    "    def normalize(self, df):\n",
    "        \"\"\" Normalize all columns in a dataframe (cohort_df) to mean 0 and std 1\n",
    "        \"\"\"\n",
    "        logging.debug(\"Normalizing features\")\n",
    "        scaler = StandardScaler()\n",
    "        x = df.values #returns a numpy array\n",
    "        x_scaled = scaler.fit_transform(x)\n",
    "        df[:] = x_scaled\n",
    "        return df\n",
    "            \n",
    "    def load_cohort_features(self):\n",
    "        \"\"\" Load all cohort-level features from file and preprocess\n",
    "        Args:\n",
    "            params:  dictionary of parameters\n",
    "        Return:\n",
    "            cohort_df:  (pandas.DataFrame)  All cohort-level data indexed by encounter ID\n",
    "        \"\"\"\n",
    "        logging.debug(\"Loading cohort features\")\n",
    "        cohort_df = pd.read_hdf(os.path.join(self.params.project_dir, self.params.data_dir, self.params.cohort_file),\n",
    "                                start=0, \n",
    "                                stop=-1) # note: loading all features and dropping after cleaning sim/src\n",
    "        cohort_df.columns = [c.split(':')[1] for c in cohort_df.columns] # clean sim/src headers for fuzzed dataset\n",
    "        cohort_df.set_index(\"ENCOUNTER_ID\", inplace=True)\n",
    "        cohort_df = cohort_df.loc[:, [self.params.label]+self.params.cohort_features]\n",
    "        logging.debug(\"Loaded {} samples from cohort-level data\".format(cohort_df.shape))\n",
    "        \n",
    "        # Drop rows with duplicate Encounter IDs\n",
    "        cohort_df = cohort_df[~cohort_df.index.duplicated(keep='first')]\n",
    "        logging.debug(\"{} samples after dropping duplicate encounter IDs\".format(cohort_df.shape))\n",
    "        \n",
    "        # Select out the rows for encounter IDs in the current CV split\n",
    "        cohort_df = cohort_df.loc[self.sample_ids, :]\n",
    "        logging.debug(\"Loaded cohort data:  {}\\n{}\".format(cohort_df.shape, cohort_df[:5]))\n",
    "        logging.debug(\"Cohort datatypes:\\n{}\".format(cohort_df.dtypes))\n",
    "        \n",
    "        # Drop rows with NaN\n",
    "        cohort_df.dropna(axis='index', how='any', inplace=True)\n",
    "        logging.debug(\"{} samples after dropping rows with missing values\".format(len(cohort_df)))\n",
    "        \n",
    "        # Expand categorical features with dummy indicators\n",
    "        categorical_features = [c for ci, c in enumerate(cohort_df.columns) if cohort_df.dtypes[ci]==object]\n",
    "        if categorical_features:\n",
    "            logging.debug(\"Expanding categorical variables to dummy indicators:  {}\".format(categorical_features))\n",
    "            cohort_df = pd.get_dummies(cohort_df, columns=categorical_features).astype(np.float32)\n",
    "            logging.debug(\"Expanded/binarized data:\\n{}\".format(cohort_df[:5]))\n",
    "        else:\n",
    "            logging.debug(\"No categorical variables to expand\")\n",
    "        \n",
    "        # Return the expanded cohort data, with labels (targets, Y) included\n",
    "        return cohort_df\n",
    "    \n",
    "    def load_hourly_features(self, sample_ids, location_column=\"LOCATION\", index_column=\"LAPS2_TS\"):\n",
    "        \"\"\" Load hourly data for all patients listed in self.sample_ids\n",
    "        \n",
    "        Params:\n",
    "            sample_ids: (list)  Which sample IDs to load data for\n",
    "        Returns:\n",
    "            hourly_dataframes: (dictionary of DataFrame, keyed by sample_id)\n",
    "            global_time_indices:  pd.DateTimeIndex?  hourly indices over all give sample IDs\n",
    "        \"\"\"\n",
    "        hourly_dataframes = {}\n",
    "        \n",
    "        for sample_id in sample_ids:\n",
    "            filepath = os.path.join(self.params.project_dir,\n",
    "                                    self.params.sample_dir,\n",
    "                                    \"itan_hourly_enc_{}.h5\".format(sample_id))\n",
    "\n",
    "            # Load patient hourly data\n",
    "            sample_hourly_df = pd.read_hdf(filepath, key=\"hourly\").reset_index()\n",
    "            sample_hourly_df.drop('index', axis=1, inplace=True)\n",
    "            sample_hourly_df.columns = [c.split(':')[1] for c in sample_hourly_df.columns] # clean sim/src headers for fuzzed dataset\n",
    "\n",
    "            # Set index to start of each hour (rounded to :00) and drop unused hourly features\n",
    "            sample_hourly_df[index_column] =  pd.to_datetime(sample_hourly_df[index_column]).dt.round('1H')  \n",
    "            sample_hourly_df.set_index(index_column, inplace=True)\n",
    "            sample_hourly_df = sample_hourly_df.loc[:, [location_column]+self.params.hourly_features]\n",
    "            logging.debug(\"Sample {} hourly dataframe:\\n{}\".format(sample_id, sample_hourly_df[:5]))\n",
    "            \n",
    "            sample_pad = min(params.pad_length, len(sample_hourly_df))\n",
    "            \n",
    "            # Retain hourly data in CPU memory\n",
    "            hourly_dataframes[sample_id] = sample_hourly_df\n",
    "            \n",
    "        # Check time frames and build global time index\n",
    "        start_stop_times = pd.DataFrame(data=[[sample_hourly_df.index.min(), sample_hourly_df.index.max()]\n",
    "                                              for sample_id, sample_hourly_df in hourly_dataframes.items()],\n",
    "                                        columns=[\"Start\", \"Stop\"])\n",
    "        \n",
    "        logging.debug(\"Start/stop times for each sample ID:\\n{}\".format(start_stop_times))\n",
    "        global_time_indices = pd.date_range(start=start_stop_times[\"Start\"].min(),\n",
    "                                            end=start_stop_times[\"Stop\"].max(),\n",
    "                                            freq=\"1H\")\n",
    "        logging.debug(\"Global time indices over {} hours:\\n{}\".format(len(global_time_indices), global_time_indices[:5]))\n",
    "        \n",
    "        return hourly_dataframes, global_time_indices\n",
    "        \n",
    "    def prebuild_patient_locations(self, sample_ids, hourly_dataframes, global_time_indices):\n",
    "        \"\"\" Build matrix of patient locations at each time\n",
    "            Note:  assumes single facility (FAC_ID not used)\n",
    "            \n",
    "        Returns:\n",
    "            unique_locations: (list) string values of all unique unit locations\n",
    "            hourly_patient_locations: (pd.DataFrame)  shape=(time_max x num_patients)\n",
    "        \"\"\"\n",
    "        logging.debug(\"Pre-building all patient locations in global time index\")\n",
    "        # Build list of all possible locations.  Location matrix values will be indices into this list\n",
    "#         locations = set([])\n",
    "#         for sample_id in sample_ids:\n",
    "#             locations += hourly_dataframes[sample_id][\"LOCATION\", :].unique()\n",
    "#         locations = list(locations)\n",
    "        \n",
    "        hourly_patient_locations = pd.DataFrame(columns=sample_ids, index=global_time_indices)\n",
    "        with tqdm(total=len(sample_ids), desc=\"Sample IDs\") as ts:\n",
    "            for sample_id in sample_ids:\n",
    "                sample_times = hourly_dataframes[sample_id].index\n",
    "                hourly_patient_locations.loc[sample_times, sample_id] = hourly_dataframes[sample_id][\"LOCATION\"]\n",
    "                ts.update()\n",
    "            \n",
    "        logging.debug(\"Built patient locations: {}:\\n{}\".format(hourly_patient_locations.shape, hourly_patient_locations[:3]))\n",
    "        return hourly_patient_locations\n",
    "    \n",
    "    def prebuild_patient_associations(self, sample_ids, hourly_patient_locations, history_hours=0):\n",
    "        \"\"\" Prebuild array for which patients are associated with which.\n",
    "            Based on simultaneous location in same facility/unit, or prior occupation within a fixed time window\n",
    "        \n",
    "        Params:\n",
    "            sample_ids:  (list)  which patients for which to find associated other patients (e.g. the batch samples)\n",
    "            hourly_patient_locations:  (DataFrame)  the patient locations for all samples\n",
    "            history_hours:  (int) number of hours to lookback when deciding if patients are associated\n",
    "                            default 0 to require simultaneous occupancy of same unit\n",
    "        \n",
    "        Returns:\n",
    "            associations:  (DataFrame of bool)  indicators for which patients are associated with the given sample patients\n",
    "        \"\"\"\n",
    "        logging.debug(\"Pre-building all patient associations\")\n",
    "        all_sample_ids = hourly_patient_locations.columns\n",
    "        associations = pd.DataFrame(0, dtype=np.bool, index=sample_ids, columns=all_sample_ids)\n",
    "        \n",
    "        with tqdm(total=len(sample_ids), desc=\"Sample IDs\") as ts:\n",
    "            for sample_id in sample_ids:\n",
    "                # Create location history matching array\n",
    "                # Note:  len(sample_ids) <= len(all_sample_ids)\n",
    "\n",
    "                # Check which patients were in a matching location at some point in the history limit\n",
    "\n",
    "\n",
    "                # TODO:  Haven't implemented historical occupation yet\n",
    "                if (history_hours != 0): raise NotImplementedError\n",
    "\n",
    "                all_locations = hourly_patient_locations.values\n",
    "                logging.debug(\"All locations:  {}\\n{}\".format(hourly_patient_locations.shape, hourly_patient_locations[:3]))\n",
    "                sample_locations = np.expand_dims(hourly_patient_locations.loc[:, sample_id].values, axis=1)\n",
    "                logging.debug(\"\\nSample {} Locations:  {}\\n{}\".format(sample_id, sample_locations.shape, sample_locations[:3]))\n",
    "                matched_locations = (hourly_patient_locations.values == sample_locations)\n",
    "                logging.debug(\"\\nMatched Location Indicators:  {}\\n{}\".format(matched_locations.shape, matched_locations[:3]))\n",
    "                associated_patient_mask = np.any(matched_locations, axis=0)\n",
    "                logging.debug(\"\\nMatched Location at any Time:  {}\\n{}\".format(associated_patient_mask.shape, associated_patient_mask[:10]))\n",
    "                \n",
    "                # Mask out which patients simultaneously occupy a unit with the current sample at any point in time\n",
    "#                 associated_patient_mask = (hourly_patient_locations == hourly_patient_locations[sample_id]).any(axis='index')\n",
    "                associations.loc[sample_id, all_sample_ids[associated_patient_mask]] = True\n",
    "                ts.update()\n",
    "                \n",
    "        logging.debug(\"Patient associations: {}\\n{}\".format(associations.shape, associations[:3]))\n",
    "        association_counts = associations.sum(axis=1)\n",
    "        logging.debug(\"Average of {}+/={:.2f} associations per sample\".format(association_counts.mean(), association_counts.std()))\n",
    "        return associations\n",
    "        \n",
    "    def get_associated_patients(self, sample_id, associations):\n",
    "        \"\"\" Get the sample IDs for all patients associated with sample_id\n",
    "            Uses the prebuilt associations matrix\n",
    "            \n",
    "        Args:\n",
    "            sample_id:  (string)\n",
    "            \n",
    "        Returns:\n",
    "            associated_patients:  (list) sample IDs associated with input sample ID\n",
    "        \"\"\"\n",
    "        all_sample_ids = associations.columns\n",
    "        associated_mask = associations.loc[sample_id, :].values\n",
    "        logging.debug(\"Mask:  {}\\n{}\".format(associated_mask.shape, associated_mask[:10]))\n",
    "        associated_patients = all_sample_ids[associated_mask]\n",
    "        logging.debug(\"Patients:  {}\\n{}\".format(associated_patients.shape, associated_patients[:10]))\n",
    "        associated_patients = [sample_id] + [p for p in associated_patients if p != sample_id]\n",
    "#         associated_indices = np.where(associations[sample_id, :].values == 1)[0]\n",
    "#         associated_patients = [sample_id] + [all_sample_ids[ai] for ai in associated_indices\n",
    "#                                              if all_sample_ids[ai] != sample_id]\n",
    "        logging.debug(\"Re-ordered Patients:  {}\\n{}\".format(len(associated_patients), associated_patients[:10]))\n",
    "        return associated_patients  # first sample ID in list is always the input sample ID\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sample_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Retrieve a single sample and bundle the data for subsequent batching (performed by DataLoader)\n",
    "        \"\"\"\n",
    "        # Get encounter ID (CSN)\n",
    "        sample_id = self.sample_ids[index]\n",
    "        logging.debug(\"Retrieving sample:  {}\".format(sample_id))\n",
    "        \n",
    "        # Get all associated patients\n",
    "        # Defined as anyone in a unit with a batch patient simultaneously, or within params.association_history hours prior\n",
    "        associated_patients = self.get_associated_patients(sample_id, self.patient_associations)\n",
    "        logging.debug(\"Found {} associated patients\".format(len(associated_patients)))\n",
    "        \n",
    "        # Create mask/index for positions of batch patient(s) within batch-associated patients\n",
    "        # No longer necessary if duplicating batch-associated data?\n",
    "        sample_index = 0\n",
    "        \n",
    "        # Get all static data (cohort-level, e.g. non-hourly)\n",
    "        sample_static_features = self.cohort_df.loc[associated_patients, :].values\n",
    "        logging.debug(\"Associated patients' static features: {}\".format(sample_static_features.shape))\n",
    "        \n",
    "        # Bundle all hourly data for associated patients\n",
    "        # Initialize sample arrays\n",
    "        logging.debug(\"Reformatting hourly features...\")\n",
    "        sample_hourly_features = np.full(fill_value=self.params.pad_value,\n",
    "                                          shape=(len(associated_patients), self.params.pad_length, len(self.params.hourly_features)),\n",
    "                                          dtype=np.float32)\n",
    "        sample_hourly_lengths = np.zeros(len(associated_patients), dtype=np.int32)\n",
    "        \n",
    "        # Fill sample arrays:  Pad and mask time dimension to a uniform shape\n",
    "        for asi, associated_sample_id in enumerate(associated_patients):\n",
    "            associated_hourly_df = self.hourly_dataframes[associated_sample_id].loc[:, self.hourly_features]\n",
    "            logging.debug(\"Associated sample {} hourly DF:  {}\".format(associated_sample_id, associated_hourly_df.shape))\n",
    "            sample_hourly_lengths[asi] = min(self.params.pad_length, len(associated_hourly_df))\n",
    "            logging.debug(\"Sample pad length: {}\".format(sample_hourly_lengths[asi]))\n",
    "            clipped_df = associated_hourly_df.iloc[:sample_hourly_lengths[asi], :]\n",
    "            logging.debug(\"Hourly data clipped to pad length:  {}\\n{}\".format(clipped_df.shape, clipped_df[:3]))\n",
    "            sample_hourly_features[asi, :sample_hourly_lengths[asi], :] = clipped_df\n",
    "            logging.debug(\"All data clipped and padded to pad length:  {}\\n{}\".format(sample_hourly_features.shape, sample_hourly_features[:3]))\n",
    "        logging.debug(\"Associated patients' hourly features: {}\\n{}\".format(sample_hourly_features.shape, sample_hourly_features[:3]))\n",
    "                \n",
    "        # Create mask for associated patient/unit location matching\n",
    "        # Allows scattering the short-term hourly data on a per-sample timescale to the global time indices\n",
    "        # Drop head and tail of all NaN (leftover global time indices for non-associated patients)\n",
    "        logging.debug(\"Reformatting hourly locations\")\n",
    "        sample_hourly_locations = self.hourly_patient_locations.loc[:, associated_patients]\n",
    "        first_idx = sample_hourly_locations.first_valid_index()\n",
    "        last_idx = sample_hourly_locations.last_valid_index()\n",
    "        sample_hourly_locations = sample_hourly_locations.loc[first_idx:last_idx].values\n",
    "        logging.debug(\"Hourly locations f:  {}\\n{}\".format(sample_hourly_locations.shape, sample_hourly_locations[:3]))\n",
    "        \n",
    "        # Bundle all arrays/values for batching, and return\n",
    "        sample = (sample_static_features, sample_hourly_locations, sample_hourly_features, sample_hourly_lengths)\n",
    "        return sample\n",
    "    \n",
    "# for reference, from forward()\n",
    "# Extract bundled batch data\n",
    "#         batch_samples, patient_inputs = inputs\n",
    "#         patient_static_features, patient_locations, patient_hourly_features, patient_hourly_lengths = patient_inputs\n",
    "        \n",
    "test_sample_ids = pd.Series([330000197395, 330000372959, 330000804075, 330000699855, 330000920644, 330000322138, 330000881688, 330000563672, 330000535940, 330000503459])\n",
    "test_dataset = ITANStrainDataset(test_sample_ids, params)\n",
    "test_batch = next(iter(test_dataset))\n",
    "logging.info(\"Dataset created!\")\n",
    "\n",
    "# # Construct Skorch datasets for training\n",
    "# logging.info(\"Constructing Strain datasets for each cross-validation split.\")\n",
    "# datasets = {}\n",
    "# for split in [\"train\", \"val\"]:\n",
    "#     logging.info(\"Building dataset for {} split\".format(split))\n",
    "#     datasets[split] = ITANStrainDataset(cv_split_samples[split], params)\n",
    "#     logging.info(\"{} split:  {} samples:\\n{}\\nX: {}\\nY: {}\".format(split, len(cv_split_samples[split]), cv_split_samples[split][:3], None, datasets[split].Y.shape))\n",
    "    \n",
    "#     logging.info(\"y values:  {}:  mean {:.2f} +/- {:.2f}\".format(\n",
    "#         datasets[split].Y.shape,\n",
    "#         np.mean(datasets[split].Y),\n",
    "#         np.std(datasets[split].Y)))  \n",
    "    \n",
    "#     # Test iteration through dataset\n",
    "#     if False:\n",
    "#         with tqdm(total=len(datasets[split]), desc=\"Testing dataset iterator\") as st:\n",
    "#             for sample in datasets[split]:\n",
    "#                 st.update()\n",
    "# logging.info(\"\\nDatasets defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.info(len(test_dataset))\n",
    "# logging.info(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.info(list(cv_split_samples[split][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batched static features:   (1, 5)\n",
      "[[ 0.06796826 -0.8852548   0.6510309  -1.          1.        ]]\n",
      "\n",
      "Batched hourly locations:  (81, 1)\n",
      "ENCOUNTER_ID        330000197395\n",
      "2018-06-13 05:00:00         WARD\n",
      "2018-06-13 06:00:00         WARD\n",
      "2018-06-13 07:00:00         WARD\n",
      "2018-06-13 08:00:00         WARD\n",
      "2018-06-13 09:00:00         WARD\n",
      "2018-06-13 10:00:00         WARD\n",
      "2018-06-13 11:00:00         WARD\n",
      "2018-06-13 12:00:00         WARD\n",
      "2018-06-13 13:00:00         WARD\n",
      "2018-06-13 14:00:00         WARD\n",
      "2018-06-13 15:00:00         WARD\n",
      "2018-06-13 16:00:00         WARD\n",
      "2018-06-13 17:00:00         WARD\n",
      "2018-06-13 18:00:00         WARD\n",
      "2018-06-13 19:00:00         WARD\n",
      "2018-06-13 20:00:00         WARD\n",
      "2018-06-13 21:00:00         WARD\n",
      "2018-06-13 22:00:00         WARD\n",
      "2018-06-13 23:00:00         WARD\n",
      "2018-06-14 00:00:00         WARD\n",
      "2018-06-14 01:00:00         WARD\n",
      "2018-06-14 02:00:00         WARD\n",
      "2018-06-14 03:00:00         WARD\n",
      "2018-06-14 04:00:00         WARD\n",
      "2018-06-14 05:00:00         WARD\n",
      "2018-06-14 06:00:00         WARD\n",
      "2018-06-14 07:00:00         WARD\n",
      "2018-06-14 08:00:00         WARD\n",
      "2018-06-14 09:00:00         WARD\n",
      "2018-06-14 10:00:00         WARD\n",
      "...                          ...\n",
      "2018-06-15 08:00:00         WARD\n",
      "2018-06-15 09:00:00         WARD\n",
      "2018-06-15 10:00:00         WARD\n",
      "2018-06-15 11:00:00         WARD\n",
      "2018-06-15 12:00:00         WARD\n",
      "2018-06-15 13:00:00         WARD\n",
      "2018-06-15 14:00:00         WARD\n",
      "2018-06-15 15:00:00         WARD\n",
      "2018-06-15 16:00:00         WARD\n",
      "2018-06-15 17:00:00         WARD\n",
      "2018-06-15 18:00:00         WARD\n",
      "2018-06-15 19:00:00         WARD\n",
      "2018-06-15 20:00:00         WARD\n",
      "2018-06-15 21:00:00         WARD\n",
      "2018-06-15 22:00:00         WARD\n",
      "2018-06-15 23:00:00         WARD\n",
      "2018-06-16 00:00:00         WARD\n",
      "2018-06-16 01:00:00         WARD\n",
      "2018-06-16 02:00:00         WARD\n",
      "2018-06-16 03:00:00         WARD\n",
      "2018-06-16 04:00:00         WARD\n",
      "2018-06-16 05:00:00         WARD\n",
      "2018-06-16 06:00:00         WARD\n",
      "2018-06-16 07:00:00         WARD\n",
      "2018-06-16 08:00:00         WARD\n",
      "2018-06-16 09:00:00         WARD\n",
      "2018-06-16 10:00:00         WARD\n",
      "2018-06-16 11:00:00         WARD\n",
      "2018-06-16 12:00:00          NaN\n",
      "2018-06-16 13:00:00         WARD\n",
      "\n",
      "[81 rows x 1 columns]\n",
      "\n",
      "Batched hourly features:   (1, 12, 1)\n",
      "[[[ 30.]\n",
      "  [ 30.]\n",
      "  [ 30.]\n",
      "  [ 24.]\n",
      "  [ 91.]\n",
      "  [ 56.]\n",
      "  [ 57.]\n",
      "  [ 53.]\n",
      "  [ 53.]\n",
      "  [ 60.]\n",
      "  [ 61.]\n",
      "  [ 68.]]]\n",
      "\n",
      "Batched hourly lengths:    (1,)\n",
      "[12]\n"
     ]
    }
   ],
   "source": [
    "batch_static_features, batch_hourly_locations, batch_hourly_features, batch_hourly_lengths = test_batch\n",
    "logging.info(\"Batched static features:   {}\\n{}\".format(batch_static_features.shape, batch_static_features))\n",
    "logging.info(\"\\nBatched hourly locations:  {}\\n{}\".format(batch_hourly_locations.shape, batch_hourly_locations))\n",
    "logging.info(\"\\nBatched hourly features:   {}\\n{}\".format(batch_hourly_features.shape, batch_hourly_features))\n",
    "logging.info(\"\\nBatched hourly lengths:    {}\\n{}\".format(batch_hourly_lengths.shape, batch_hourly_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader\n",
    "\n",
    "1.  Define dataset (single facility per epoch)\n",
    "2.  Load all static data to CPU memory\n",
    "3.  Load all hourly data to CPU memory\n",
    "4.  Randomly select batch of B patients\n",
    "5.  Find all associated patients for the batch\n",
    "        a. Look at full location (unit) history for each batch patient\n",
    "        b. Find all other patients who were in same unit as the batch patient at any time\n",
    "        c. (future) All patients in same unit less than N hours prior to batch patient\n",
    "6.  Reformat all batch data to a common time scale\n",
    "7.  Create location masks for NN grouping\n",
    "8.  Push batch data to GPU and Tensor format for PyTorch\n",
    "\n",
    "\n",
    "## To Do\n",
    "8.  Feed batch data through Neural Network and compute loss\n",
    "9.  Train and evaluate on full dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
